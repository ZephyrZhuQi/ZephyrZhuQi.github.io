2019/8/12

1. è·‘QAnet

<br/>

2. VCRçš„knowledge baseç±»å‹

* co-occurence
* ç›¸å¯¹ä½ç½®
* ...
* åŠ¨ä½œ
* æ¯”è¾ƒçº§

<br/>

3. VCRæ•°æ®é›†ä»¥åŠmodel

<br/>

4. Conceptnetæœ‰å¾ˆå¤šnoise

DBpediaä¸å¤ªå¸¸è¯†ï¼Œè™½ç„¶å¾ˆç²¾å‡†

<br/>

5. Visual Genome 

scene graph

å›¾åƒè¡¨ç¤ºæˆå›¾ï¼ˆå›¾åƒå¤„ç†çš„åœ£æ¯ï¼‰

<br/>

6. å…³ç³»å›¾çš„å½¢å¼/ä¸‰å…ƒç»„çš„å½¢å¼

ä¸‰å…ƒç»„ï¼ˆæ‰©å±•æ€§å¥½ï¼‰

<br/>

2019/8/29

cvä»»åŠ¡ï¼š

1. ç¬¬ä¸€é˜¶æ®µImage â†’ Image(å›¾åƒå¤„ç†)

* Deblurï¼ˆå»é™¤æ¨¡ç³Šï¼‰

* Denoiseï¼ˆé™å™ªï¼‰

* Dehazeï¼ˆé™¤éœ¾ï¼‰

* Super Resolutionï¼ˆè¶…åˆ†è¾¨ç‡ï¼‰

2. ç¬¬äºŒé˜¶æ®µImage â†’ Labels + Coordinates

* segmentation(åˆ†å‰²)
* detection(æ£€æµ‹)
* tracking(è·Ÿè¸ª)

3.ç¬¬ä¸‰é˜¶æ®µImage â†’ Semantics/Language

* scene graph generation
* vision-language interaction



1. ConceptNetä¸‹è½½ä¸‹æ¥å¹¶ç†è§£

<br/>

å¼•å…¥å¤–éƒ¨çŸ¥è¯†çš„æ–¹æ³•ï¼š

âˆ’ 1. Representation based
âˆ’ 2. Model based
2.1 knowledge distillation
2.2 pretraining & fine-tuning
âˆ’ 3. Memory based

2. introduce external knowledge 

Representation based

Distributed Representationsç»Ÿè®¡å­¦çš„çŸ¥è¯† bag of words

ç”¨å›½å®¶ã€é€‰ä¸¾çš„æ¦‚ç‡åˆ†å¸ƒæ¥è¡¨ç¤ºâ€œæ”¿æ²»â€

<br/>

3. Word2Vec GloVe

Paragraph2Vec Doc2Vec

<br/>

4. Multimodal Compact Bilinear  å…³æ³¨ä¸€ä¸‹ç”¨äº†GloVe

<br/>

5. Visual Relationship Detection with Language Priors, Lu et al., ECCV, 2016

è´å¶æ–¯å…¬å¼ å…ˆéªŒæ¦‚ç‡ åéªŒæ¦‚ç‡ Word2Vecå…¸å‹åº”ç”¨

<br/>

6. Visual Relationship Detection with Internal and External Linguistic Knowledge Distillation, Yu et al., ICCV, 2017

å…ˆéªŒä¿¡æ¯ enforce student æ ·æœ¬åˆ†å¸ƒæ¥è¿‘teacheræ ·æœ¬åˆ†å¸ƒ

å®é™…ä¸Šåšçš„å°±æ˜¯åœ¨å­¦ä¹ å¸¸è¯† teacher&studentåˆ†å¼€è®­ç»ƒ

<br/>

7. Learning to Detect Human-Object Interactions with Knowledge, Xu et al., CVPR, 2019

å…³æ³¨å¼•çš„å’Œè¢«å¼•çš„ top conference

teacher&studentåŒæ—¶è®­ç»ƒ 

ç»¿è‰²æ˜¯å†…éƒ¨çŸ¥è¯†targetï¼Œçº¢è‰²æ˜¯å¤–éƒ¨çŸ¥è¯†ï¼Œé»„è‰²æ˜¯shareçŸ¥è¯†ï¼Œå†…éƒ¨å’Œå¤–éƒ¨ä¹‹é—´å›¾å·ç§¯

<br/>

8. VL-BERT: PRE-TRAINING OF GENERIC VISUALLINGUISTIC REPRESENTATIONS, Su et al., ArXiv, 2019

upstream targetå’Œdownstream targetéƒ½å¿…é¡»æ˜¯è·¨åª’ä½“ä»»åŠ¡ï¼Œæ˜¯ä¸€ä¸ªè‡´å‘½ç¼ºé™·ï¼Œdatasetæ”¶é›†å›°éš¾

conceptual captions(Sharma et al., 2018)çœ‹ä¸€ä¸‹

<br/>

9. CycleGAN unpaired data

åˆæˆæ•°æ® æœ‰å¾ˆå¤šæ¸…æ™°çš„å›¾ç‰‡&æ¨¡ç³Šçš„å›¾ç‰‡

semi-supervision pair&unpair

<br/>

10. Dynamic Memory Networks for Visual and Textual Question Answering, Xiong et al., ICML, 2016

inputåˆ†æˆmemoryä¸­çš„å•å…ƒ

external knowledgeå¯ä»¥ç”¨è¿™ä¸ªæ¡†æ¶

<br/>

11. FVQA,UIUCè·Ÿè¿›

inferenceçš„è¿‡ç¨‹ï¼Œquery knowledge base(Memory Network)

<br/>

---

# From Recognition to Cognition: Visual Commonsense Reasoning(2018)

### ç®€ä»‹

ä¸»è¦çš„å››ä¸ªè´¡çŒ®ï¼š

1. æè¿°ä»»åŠ¡ï¼šVCR
2. å¼•è¿›æ•°æ®é›†ï¼šVCR
3. ç”Ÿæˆæ•°æ®é›†çš„æ–¹æ³•ï¼šAdversarial Matching
4. æ–°çš„æ¨ç†æœºå™¨ï¼šR2Cç½‘ç»œ



### æ•°æ®æ”¶é›†

ç”¨ï¼­ask-RCNNæ£€æµ‹åˆ°object tag



### Adversarial Matching

a new method that allows for any â€˜language generationâ€™ dataset be turned into multiple choice test

Here, we employ state-of-the-art models for Natural Language Inference: BERT [15] and ESIM+ELMo
[10, 57], respectively.



### Recognition to Cognition Networks

1. groundå±‚ï¼šè®©é—®é¢˜å’Œç­”æ¡ˆä¸­æŒ‡ä»£çš„ç‰©ä½“è½åœ°ï¼ˆä¸å›¾ç‰‡å¯¹åº”èµ·æ¥ï¼‰

   å­¦ä¹ åˆ°image-languageè”åˆè¡¨è¾¾ï¼Œæ ¸å¿ƒæ˜¯åŒå‘LSTM

   CNNå­¦å›¾åƒfeature,Roi-Aligned

   BERTå­¦æ–‡å­—feature

2. contextualizeå±‚ï¼šé—®é¢˜ã€ç­”æ¡ˆã€å›¾ç‰‡å½¢æˆä¸Šä¸‹æ–‡ï¼ˆç»“åˆåœ¨ä¸€èµ·ï¼‰

   ç”¨attentionæœºåˆ¶è·å¾—æƒé‡ã€‚

3. reasonå±‚ï¼šæ¨ç†å›¾ç‰‡åŒºåŸŸã€é—®é¢˜ã€ç­”æ¡ˆé—´çš„å…³ç³»

   æŠŠresponse, attended query and objectså–‚ç»™LSTM, åœ¨æ¯ä¸ªtimestepä¸é—®é¢˜å’Œç­”æ¡ˆconcatenateèµ·æ¥ï¼Œç»™MLPã€‚


ROI Align æ˜¯åœ¨Mask-RCNNè¿™ç¯‡è®ºæ–‡é‡Œæå‡ºçš„ä¸€ç§åŒºåŸŸç‰¹å¾èšé›†æ–¹å¼, å¾ˆå¥½åœ°è§£å†³äº†ROI Poolingæ“ä½œä¸­ä¸¤æ¬¡é‡åŒ–é€ æˆçš„åŒºåŸŸä¸åŒ¹é…(mis-alignment)çš„é—®é¢˜ã€‚å®éªŒæ˜¾ç¤ºï¼Œåœ¨æ£€æµ‹ä»»åŠ¡ä¸­å°† ROI Pooling æ›¿æ¢ä¸º ROI Align å¯ä»¥æå‡æ£€æµ‹æ¨¡å‹çš„å‡†ç¡®æ€§ã€‚



### ç»“æœå’Œå‰¥ç¦»æµ‹è¯•

The model suffers most when using GloVe representations instead of BERT: a loss of 24%. This suggests that strong textual representations are crucial to VCR performance.




# FVQA: Fact-based Visual Question Answering(2017)(TPAMI2018)

ä¸»è¦è´¡çŒ®ï¼š

1. æ•°æ®é›†ï¼šFVQA
2. ä¸€ä¸ªè§£å†³é—®é¢˜çš„model



åœ¨CVPR2019ï¼ˆVisual Question Answering as Reading Comprehensionï¼‰é‚£ç¯‡è®ºæ–‡ä¸­æåˆ°è¿™ç¯‡è®ºæ–‡ï¼š

> Wang et al. [29] introduced the â€œFact-based VQA (FVQA)â€ problem and proposed a semantic parsing based method for supporting facts retrieval. 

> This method is vulnerable to misconceptions caused by synonyms and homographs(åŒå½¢è¯).



# Visual Question Answering as Reading Comprehension(2018)(CVPR2019)


1)represent image content explicitly by natural language and solve VQA as a reading comprehension problem

2)Two types of VQA models are proposed to address the open-end VQA and the multiple-choice VQA respectively

3)address knowledge based VQA

### Introduction

* VQA: visual question answering

* TQA: textual question answering/ machine reading comprehension

 (CNNs) to represent images and  (RNNs) to represent sentences or phrases

* The extracted visual and textual feature vectors are then jointly embedded by concatenation, element-wise sum or product to infer the answer.

* Multimodal Compact Bilinear pooling method (MCB) for VQA

* embed knowledge in memory slots and incorporated external knowledge with image, question and answer features by Dynamic Memory Networks (DMN)

### Related Work  

2.1. Joint embedding  

2.2. Knowledge-based VQA

ç°æœ‰çš„ç ”ç©¶åŒ…æ‹¬ä¸¤ä¸ªæ–¹æ³•ï¼šsemantic parsing or information retrieval

2.3. Textual Question Answering

end-to-end neural network models and attention mechanism, such as DMN [17], r-net [31], DrQA [6], QANet [36], and most recently BERT [7]

### VQA Models

3.1. QANet

It consists of embedding block, embedding encoder, context-query attention block, model encoder and output layer.

3.2. Open-ended VQA model

Wang et al. [29] proposed to query the knowledge bases according to the estimated query types and visual concepts detected from the image.  

Heuristic keyword matching: no

Rather than apply the heuristic matching approach which is vulnerable to homographs and synonyms, here we make use of all the retrieved candidate supporting facts as context

3.3. Multiple-choice VQA model

4. Experiments

4.1. Datasets  

4.2. Implementation Details  

4.2.1 Results Analysis on FVQA

4.2.2 Results Anslysis on Visual Genome QA

4.2.3 Results Analysis on Visual7W

5. Conclusion

[VALSEä¼šè®®ppt](https://blog.csdn.net/TheDayIn_CSDN/article/details/90513072)





  

# Never ending language learning

ä»¥å‰çš„æŠ€æœ¯ï¼šsupervised function approximation

ç›¸å…³çš„å·¥ä½œ:

e.g., life long learning (Thrun and Mitchell 1995), and that 

learn to learn (Thrun and Pratt 1998),

work on multitask transfer learning (Caruana 1997)


---
# Generative Adversarial Nets (NIPS 2014)
<!-- ç›¸å½“äºåå·ç§¯ -->

<!-- Generatoråƒä¸€ä¸ªä½ç»´åº¦çš„å‘é‡ï¼Œç”Ÿæˆä¸€ä¸ªé«˜ç»´åº¦çš„å‘é‡(image,æ¯ä¸€ä¸ªç»´åº¦æ˜¯ä¸€ä¸ªpixel)
Discriminatoråƒä¸€ä¸ªé«˜ç»´åº¦çš„å‘é‡ï¼Œè¾“å‡ºä¸€ä¸ªscalar,æ•°å€¼è¶Šå¤§è¡¨æ˜è¶ŠçœŸå®
train discriminator: regression or classificationå¥½çš„æ˜¯1ï¼Œåçš„æ˜¯0 -->

<!-- è¾¾æˆçº³ä»€å‡è¡¡ï¼š -->

<!-- ç‰©ç«å¤©æ‹©ï¼ŒçŒé£Ÿè€…å’ŒçŒç‰©ï¼›åšå¼ˆï¼›

è€å¸ˆå’Œå­¦ç”Ÿï¼›ååŒæ¼”åŒ–ï¼›çˆ±ä¸å’Œå¹³ï¼› -->

<!-- Generatorå’ŒDiscriminatorç»„åˆèµ·æ¥æˆä¸€ä¸ªå·¨å¤§çš„networkï¼Œå›ºå®šåé¢ä¸€åŠçš„hidden layerï¼Œå¯¹å‰é¢ä¸€åŠåšGradient Ascent,å› ä¸ºè¦è¾“å‡ºè¶Šå¤§è¶Šå¥½ï¼ˆGradient Descentæ˜¯è¦è¶Šå°è¶Šå¥½ï¼‰ -->

<!-- è®­ç»ƒDiscriminatoræœ€å¤§åŒ–

è®­ç»ƒGeneratoræœ€å¤§åŒ– -->

## GAN as Structured Learning(Output is composed of components with dependency)

Structured Learning/Prediction: output a
sequence, a matrix, a graph, a tree ......

Structured Learning as One-shot/Zero-shot Learning

Generator(Bottom
Up): Learn to generate
the object at the
component level

Discriminator(Top
Down): Evaluating the
whole object, and
find the best one

Discriminator: Evaluation function, Potential
Function, Energy Function ...

discriminator to generate: graphical model 

structured learning -> graphical model -> 

Bayesian Network
(Directed Graph)

Markov Random Field
(Undirected Graph)

graph, potential function - discriminator 
(iteratively)positive + negative examples, model sample negative examples, update model 

Energy-based
Model:
http://www.cs.nyu.edu/~yann/research/ebm/
Although we do not know the distributions of ğ‘ƒ_ğº and ğ‘ƒ_ğ‘‘ğ‘ğ‘¡ğ‘,
we can sample from them.

use sample to replace expectation

<!-- ä¸‰ç§GAN

* Typical GAN

* Conditional GAN
  Conditional GAN â€“ Image-to-label åŠ ä¸ŠGANï¼Œè€ƒè™‘labelä¹‹é—´çš„dependency

  ç»™å®štextäº§ç”Ÿå›¾åƒ
  
* Unsupervised Conditional GAN

stack GANå…ˆäº§ç”Ÿå°çš„å›¾ï¼Œå†äº§ç”Ÿå¤§çš„å›¾

patch GAN Discriminatoræ¯æ¬¡æ£€æŸ¥å›¾ç‰‡çš„ä¸€å°å— -->


  



---

# Visual Relationship Detection with Internal and External Linguistic(è¯­è¨€çš„;è¯­è¨€å­¦çš„;) Knowledge Distillation(ICCV, 2017)

> å…ˆéªŒä¿¡æ¯ enforce student æ ·æœ¬åˆ†å¸ƒæ¥è¿‘teacheræ ·æœ¬åˆ†å¸ƒ
>
> å®é™…ä¸Šåšçš„å°±æ˜¯åœ¨å­¦ä¹ å¸¸è¯† teacher&studentåˆ†å¼€è®­ç»ƒ

æ•°æ®é›†ï¼šå¢ç­–å¾Visual Relationship Detection (VRD), Visual Genome datasets

ç»™å®šä¸»è¯­å’Œå®¾è¯­çš„å¯¹å­ï¼Œè®¡ç®—è°“è¯çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ

![knowledge distillation framework](./images/know_dist_frame.png)



# NETWORK COMPRESSION

â€¢ Network Pruning

After pruning, the accuracy will
drop (hopefully not too much)

Fine-tuning on training data for
recover

<!-- ä¸å°‘paperè¯æ˜åªè¦networkå¤Ÿå¤§ï¼Œgradient descentå¯ä»¥æ‰¾åˆ°global optimum -->

â€¢ Knowledge Distillation

<!-- teacher Netå’Œstudent Netå¯å¦åŒæ—¶è®­ç»ƒçš„é—®é¢˜ï¼Ÿ -->

<!-- ![knowledge](./images/knowdist.png) -->

<!-- ![knowledge](./images/knowdist1.png) -->

<!-- ![knowledge](./images/knowdist2.png) -->

â€¢ Parameter Quantization
â€¢ Architecture Design
â€¢ Dynamic Computation


---

## é—®é¢˜è§£å†³
å®‰è£…torchvisionçš„layersåˆ†æ”¯æ—¶ï¼Œå‡ºç°äº†ä¸‹åˆ—é—®é¢˜ï¼š

```bash
unable to execute '/usr/local/cuda:/bin/nvcc': No such file or directory
error: command '/usr/local/cuda:/bin/nvcc' failed with exit status 1
```

è§£å†³æ–¹æ³•ï¼Œä¿®æ”¹/etc/profileä¸­çš„ç¯å¢ƒå˜é‡

```bash
export PATH=/usr/local/cuda/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH
export CUDA_HOME=/usr/local/cuda:$CUDA_HOME
```

ç¬¬ä¸‰è¡Œæ”¹ä¸º
```bash
export CUDA_HOME=/usr/local/cuda
```

<br/>


spacy>=2.0,<2.1

en_core_web_smã€€2.0.0ç‰ˆæœ¬ä¸spacyé€‚é…

<br/>

export PYTHONPATH=/home/B/gaoye/code/r2c

<br/>



ln -s /home/B/gaoye/data/vcr/vcr1image
<br/>


/dataloaders/vcr.py:

self.h5fn = os.path.join(VCR_ANNOTS_DIR, f'{self.embs_to_load}_{self.mode}_{self.split}.h5')

print("Loading embeddings from {}".format(self.h5fn), flush=True)

<br/>





https://github.com/rowanz/r2c/issues/9


<br/>


python train.py -params models/multiatt/default.json -folder models/saves/flagship_answer
<br/>


scp -P 10035 -r gaoye@10.69.21.155:/home/B/gaoye/tool ~/tool
<br/>


---

## Unpaired Image Captioning by Language Pivoting (ECCV 2018)

Alibaba AI Labs

pivotæ ¸å¿ƒï¼Œæ¢è½´è¯­è¨€ï¼Œä½œä¸ºç¬¬ä¸‰è¯­è¨€æ²Ÿé€šä¸¤ç§è¯­è¨€ä¹‹é—´ç¿»è¯‘çš„æ¡¥æ¢

![pivot](./images/pivot.PNG)

![pivot1](./images/pivot1.png)

<!-- beam search:  -->

<!-- Vijayakumar, A.K., Cogswell, M., Selvaraju, R.R., Sun, Q., Lee, S., Crandall, D., Batra, D.: Diverse beam search: Decoding diverse solutions from neural sequence models. arXiv preprint arXiv:1610.02424 (2016)  -->

## Unpaired Image Captioning via Scene Graph Alignments(ICCV2019)




## Ranzato, M., Chopra, S., Auli, M., Zaremba, W.: Sequence level training with recurrent neural networks. In: ICLR (2016)

## Show, Tell and Discriminate: Image Captioning by Self-retrieval with Partially Labeled Data (ECCV 2018)

CUHK

centerpieceä¸­å¿ƒè£…é¥°å“ï¼›æ”¾åœ¨é¤æ¡Œä¸­å¤®çš„æ‘†é¥°;

å¯¹å¶ä»»åŠ¡ï¼š

Captioning Module 

Self-retrieval Module

Image captioning methods can be divided into three categories [49]. 

1. Template based methods [20, 29, 48] generate captions based on language templates. 

2. Search-based methods [11,13] search for the most semantically similar captions from a sentence pool. 
3. Recent works mainly focus on language-based methods with an encoder-decoder framework [7,14â€“17,28,41,43,46,47], where a convolutional neural network (CNN) encodes images into visual features, and an Long Short Term Memory network (LSTM) decodes features into sentences [41]. 

It has been shown that attention mechanisms [5,26,31,47] and high-level attributes and concepts [14,16,49,50] can help with image captioning. 

![self_retrieval](./images/self_retrieval.png)



 The word at each time step t is chosen based on the probability distribution of each word by greedy decoding or beam search.

![retrieval](./images/retrieval.PNG)

 rattan è—¤

posterior åœ¨åé¢çš„

## Unsupervised Image Caption (CVPR 2019)

è…¾è®¯AI lab 

Rochesterå¤§å­¦

warrant ä½¿æœ‰å¿…è¦

![caption](./images/unsupervi_caption.png)

![unsuper](./images/unsuper.png)

![gen](./images/gen_dis.png)



