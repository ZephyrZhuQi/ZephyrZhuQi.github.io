---
layout: post
title: Machine Learning by Hung-yi Lee
categories: CS
excerpt: Machine Learning
status: visible
---

# Introduction of Reinforcement Learning

## Scenario of Reinforcement Learning

State/Observation

State is not that f a system, but of the environment. Just the observation of the agent.

## Outline
* Policy-based (Learning an Actor)
* Value-based (Learning a Critic)

Actor + Critic: Asynchronous Advantage Actor-Critic (A3C)

Alpha Go: policy-based + value-based + model-based

Actor/Policy Action = Ï€( Observation )

## Policy-based Approach (Learning an Actor)

### Goodness of Actor
An episode is considered as a trajectory $\tau$

* $ğœ = \{ ğ‘ _1, ğ‘_1, ğ‘Ÿ_1, ğ‘ _2, ğ‘_2, ğ‘Ÿ_2, â‹¯ , ğ‘ _ğ‘‡, ğ‘_ğ‘‡, ğ‘Ÿ_ğ‘‡ \}$
* $ğ‘… (ğœ) = \sum_{t=1}^{T} r_t$
* If you use an actor to play the game, each ğœ has a
probability to be sampled
    * The probability depends on actor parameter ğœƒ: ğ‘ƒ (ğœ|ğœƒ)

$R_\theta$

$ğ‘… (ğœ)$ do not have to be differentiable
It can even be a black box.

## Value-based Approach (Learning a Critic)

## Proximal Policy Optimization (PPO)
default reinforcement learning algorithm at OpenAI

* on-policy
* off-policy

### Policy of Actor

Policy ğœ‹ is a network with parameter $\theta$

### From on-policy to off-policy (Using the experience more than once)

* On-policy: The agent learned and the agent interacting with the environment is the same.
* Off-policy: The agent learned and the agent interacting with the environment is different.

Use $ğœ‹_ğœƒ$ to collect data. When ğœƒ is updated, we have
to sample training data again. 

Goal: Using the sample from $ğœ‹_ğœƒâ€²$ to train ğœƒ. $ğœƒâ€²$ is fixed, so we can re-use the sample data. 

#### Importance Sampling

### Add Constraint

ğœƒ cannot be very different from ğœƒâ€²

Constraint on behavior not parameters

## Q-Learning
* A critic does not directly determine the action.
* Given an actor Ï€, it evaluates how good the actor is
* State value function $V^\pi(s)$
    * When using actor ğœ‹, the cumulated reward expects to be obtained after visiting state s 
The output values of a critic
depend on the actor evaluated.

### How to estimate $V^\pi(s)$
$V^\pi(s)$ is a network, regression problem
* Monte-Carlo (MC) based approach
* Temporal-difference (TD) approach

State-action value function ğ‘„^ğœ‹(ğ‘ , a)

## Actor-Critic

Asynchronous Advantage
Actor-Critic (A3C)  

â€œAsynchronous Methods for
Deep Reinforcement Learningâ€, ICML, 2016

### Review â€“ Policy Gradient
estimate the
expected value of G instead of sampling, more stable.

### Advantage Actor-Critic

