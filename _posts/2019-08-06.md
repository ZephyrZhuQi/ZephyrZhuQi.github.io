# Visual question answering: Datasets, algorithms, and future challenges - Kushal Kafle et al, CVIU 2017.

## 1 Introduction

As humans
* identify the objects in an image, 
* understand the spatial positions of these objects, 
* infer their attributes and relationships to each other, 
* reason about the purpose of each object given the surrounding context.

 Questions can
be arbitrary and they encompass many sub-problems in computer vision, e.g.,
* Object recognition - What is in the image? 
* Object detection - Are there any cats in the image? 
* Attribute classification - What color is the cat? 
* Scene classification - Is it sunny? 
* Counting - How many cats are in the image?
Beyond these, there are many more complex questions that can be asked,
* spatial relationships among objects (What is between the cat and the sofa?) 
* common sense reasoning questions (Why is the the girl crying?)

many potential applications for VQA：
* an aid to blind
and visually impaired individuals, enabling them to get information about images both on
the web and in the real world.
* improve human-computer interaction as a natural way to query visual content
* image retrieval, without using image meta-data or tags

Beyond applications, VQA is an important basic research problem.
Because a good VQA system must be able to solve many computer vision problems, it can
be considered a component of a Turing Test for image understanding [8, 9].

## 2 Vision and Language Tasks Related to VQA

The most widely used caption evaluation schemes are BLEU [21], ROUGE [22],
METEOR [23], and CIDEr [24]. With the exception of CIDEr, which was developed specifically for scoring image descriptions, all caption evaluation metrics were originally developed
for machine translation evaluation.

Dense image captioning (DenseCap) avoids the generic caption problem by annotating an image densely with short visual descriptions pertaining to small, but salient, image regions 

In conclusion, a captioning system is at liberty to arbitrarily choose the level of granularity of its image analysis which is in contrast to VQA, where the level of granularity is
specified by the nature of the question asked.

* ‘What season is this?’ will require understanding the entire scene, 
* but ‘What is the color of dog standing behind the girl with white dress?’ would require attention to specific details of the scene. 
* Moreover, many kinds of questions have specific and unambiguous answers, making VQA more amenable to automated evaluation metric than captioning. 

## 3 Datasets:
DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35]. With exception of DAQUAR, all of the datasets include images from the Microsoft Common Objects in Context (COCO) dataset [10], which
consists of 328,000 images, 91 common object categories with over 2 million labeled instances, and an average of 5 captions per image. 

## 4 Evaluation Metrics for VQA
Wu-Palmer Similarity (WUPS) [45] was proposed as an alternative to accuracy in [30]. It tries to measure how much a predicted answer differs from the ground truth based on the difference in their semantic meaning.
> However, WUPS tends to assign relatively high scores to even distant concepts, e.g., ‘raven’ and ‘writing desk’ have a WUPS score of 0.4.

multiple-choice is ill-suited for VQA because it undermines the effort by allowing a system
to peek at the correct answer.

Considerable work needs to be done to develop better tools for measuring the
semantic similarity of answers and for handling multi-word answers.

## 5 Algorithms for VQA

1) extracting image features (image featurization), 

> CNNs that are pre-trained on ImageNet, with common examples being VGGNet [1], ResNet [2], and GoogLeNet [58]

2) extracting question features (question featurization), 

> including bag-of-words (BOW), long short term memory (LSTM) encoders [59], gated recurrent units (GRU) [60], and skipthought vectors [61]

3) an algorithm that combines these features to produce an answer

> treat VQA as a classification problem. In this framework, the image and question features are the
> input to the classification system and each unique answer is treated as a distinct category.

These systems differ significantly in how they integrate the question
and image features. 

* concatenation, elementwise multiplication, or elementwise addition, and then giving them to
  a linear classifier or a neural network

* Combining the image and question features using bilinear pooling or related schemes
  in a neural network framework [46, 53, 62],
*  Having a classifier that uses the question features to compute spatial attention maps
  for the visual features or that adaptively scales local features based on their relative
  importance [49, 51, 48, 63],
*  Using Bayesian models that exploit the underlying relationships between question-image-answer feature distributions [36, 30], and
* Using the question to break the VQA task into a series of sub-problems [50, 44]



5.1 Baseline model: MLP

CNN+RNN+MLP as classifier

5.2 Bayesian and Question-Aware Models

5.3 Attention Based Models

 These models learn to ‘attend’ to the
most relevant regions of the input space

 Fewer models have also
explored incorporating attention into the text representation.

attention-weighted image features

* The Focus Regions for VQA [63] and Focused Dynamic Attention (FDA) models

* Stacked Attention Network (SAN) [49] and the Dynamic Memory Network (DMN) [52] models

  DMN:

  The dynamic memory networks are composed of 4 main modules [42] that allow independence
  in their particular implementation

  1. The input module transforms the input data into a set of vectors called “facts”
  2. The question module computes a vector representation of the question, using a gated recurrent unit (GRU, a variant of LSTM)
  3. The episodic memory module retrieves the facts required to answer the question.
  4. Finally, the answer module uses the final state of the memory and the question to predict the output with a multinomial classification for single words, or a GRU for datasets where a longer sentence is required.

* The Hierarchical Co-Attention model [54] applies attention to both the image and question to jointly reason about the two different streams of information.

5.4 Bilinear Pooling Methods

* In [46], Multimodal Compact Bilinear (MCB) pooling was proposed as a novel method
  for combining image and text features in VQA.

This idea is to approximate the outerproduct between the image and text features, allowing a deeper interaction between the two
modalities, compared to other mechanisms, e.g., concatenation or element-wise multiplication. Rather than doing the outer-product explicitly, which would be very high dimensional,
MCB does the outer-product in a lower dimensional space

* multi-modal low-rank bilinear pooling (MLB) scheme that uses the Hadamard product and a linear mapping to achieve approximate bilinear pooling.

5.5 Compositional VQA Models

* The Neural Module Network (NMN) [44, 50] framework uses external question parsers to find the subtask in the question whereas 


* Recurrent Answering Units (RAU) [55] is trained end-to-end
  and sub-tasks can be implicitly learned.

  However, they did not perform visualization or ablation studies to show how the answer
  might get refined in each time-step. 



# Visual Question Answering: A Survey of Methods and Datasets

classify methods by their mechanism to connect the visual and textual modalities

combining convolutional and recurrent neural networks to map images and questions to a common feature space

memory-augmented and modular architectures that interface with structured knowledge bases

the connection to structured knowledge bases and the use of natural language processing models

#### 2.4 Models using external knowledge bases

> Knowledge Bases (KB) such as DBpedia [5], Freebase [7], YAGO [27, 50], OpenIE [6, 17, 18], NELL [10], WebChild [73, 72], and ConceptNet [47]. 

> Each piece of knowledge, referred to as a fact, is typically represented as a triple (arg1,rel,arg2), where arg1 and arg2 represent two concepts and rel represents a relationship between them.

> The collection of such facts forms a interlinked graph, which is often described according to a Resource Description Framework (RDF [24]) specification and can be accessed by query languages such as SPARQL [62]. 



**Method**  Wang et al. [78] propose a VQA framework named “Ahab” that uses DBpedia, one of the largest
structured knowledge bases.

learn a mapping images/questions to queries over the constructed knowledge graph.

Although the questions can be provided in natural language, they are parsed using manually designed templates. An improved method named FVQA [79] uses an LSTM and a data-driven approach to learn
the mapping of images/questions to queries. 

DBpedia containing short descriptions, which are embedded into fixed-size vectors with Doc2Vec.