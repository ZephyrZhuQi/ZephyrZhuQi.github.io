---
layout: post
title: Network In Network
categories: Paper Reading
excerpt: 
status: visible
---

ICLR 2014

### 1 Introduction
Convolutional neural networks (CNNs) [1] consist of alternating convolutional layers and pooling
layers. Convolution layers take inner product of the linear filter and the underlying receptive field
followed by a nonlinear activation function at every local portion of the input. The resulting outputs
are called feature maps.

However, the data for the same concept often live on a nonlinear manifold, therefore the representations that capture these concepts are generally highly nonlinear function of the input. 

### 2 Convolutional Neural Networks
Classic convolutional neuron networks [1] consist of alternatively stacked convolutional layers and
spatial pooling layers. The convolutional layers generate feature maps by linear convolutional filters
followed by nonlinear activation functions (rectifier, sigmoid, tanh, etc.).

This linear convolution is sufficient for abstraction when the instances of the latent concepts are
linearly separable. However, representations that achieve good abstraction are generally highly nonlinear functions of the input data. In conventional CNN, this might be compensated by utilizing
an over-complete set of filters [6] to cover all variations of the latent concepts. 

In the recent maxout network [8], the number of feature maps is reduced by maximum pooling
over affine feature maps (affine feature maps are the direct results from linear convolution without applying the activation function). Maximization over linear functions makes a piecewise linear
approximator which is capable of approximating any convex functions.

### 3.1 MLP Convolution Layers

 Radial basis network and multilayer perceptron are
two well known universal function approximators.

#### 3.2 Global Average Pooling

